{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4448843c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score, precision_recall_curve\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# è¨­ç½® matplotlib ä¸­æ–‡å­—é«”\u001b[39;00m\n\u001b[32m     10\u001b[39m plt.rcParams[\u001b[33m'\u001b[39m\u001b[33mfont.sans-serif\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[33m'\u001b[39m\u001b[33mMicrosoft YaHei\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSimHei\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# æ›¿æ›ç‚ºæ‚¨ç’°å¢ƒæ”¯æ´çš„å­—é«”\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# è¨­ç½® matplotlib ä¸­æ–‡å­—é«”\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei'] # æ›¿æ›ç‚ºæ‚¨ç’°å¢ƒæ”¯æ´çš„å­—é«”\n",
    "plt.rcParams['axes.unicode_minus'] = False # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ\n",
    "\n",
    "# ===== 0ï¸âƒ£ Helper Functions (è³‡è¨Šç†µè¨ˆç®—) =====\n",
    "def calculate_entropy(series):\n",
    "    \"\"\"è¨ˆç®—è³‡è¨Šç†µ H = -SUM(p * log2(p))ï¼Œè¡¡é‡å¤šæ¨£æ€§\"\"\"\n",
    "    series = series.dropna() \n",
    "    if series.empty:\n",
    "        return 0.0\n",
    "        \n",
    "    counts = series.value_counts()\n",
    "    probabilities = counts / len(series)\n",
    "    # ä½¿ç”¨ np.log2ï¼Œè¨ˆç®—ç†µå€¼\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# ===== 1ï¸âƒ£ Load CSV =====\n",
    "# è«‹æ ¹æ“šæ‚¨çš„æª”æ¡ˆè·¯å¾‘ä¿®æ”¹ dir_path\n",
    "dir_path = \"\" \n",
    "df_txn = pd.read_csv(os.path.join(dir_path, 'acct_transaction.csv'))\n",
    "df_alert = pd.read_csv(os.path.join(dir_path, 'acct_alert.csv'))\n",
    "df_test = pd.read_csv(os.path.join(dir_path, 'acct_predict.csv'))\n",
    "\n",
    "# è½‰æ›æ—¥æœŸæ ¼å¼ä»¥ä¾¿æ¯”è¼ƒ\n",
    "df_txn['txn_date'] = pd.to_numeric(df_txn['txn_date'])\n",
    "df_alert['event_date'] = pd.to_numeric(df_alert['event_date'])\n",
    "print(\"(Finish) Load Dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e69276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ•¸æ“šæ¢ç´¢ (EDA) çµæœ ---\n",
      "ã€A.1 æ™‚æ®µæ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶æ¯å°æ™‚äº¤æ˜“ç­†æ•¸ä½”æ¯” (Alert Ratio):\n",
      "        Normal  Alert  Alert_Ratio\n",
      "hour                              \n",
      "4      41786.0  180.0     0.004289\n",
      "1      97965.0  373.0     0.003793\n",
      "23    173223.0  650.0     0.003738\n",
      "3      53453.0  200.0     0.003728\n",
      "22    206588.0  739.0     0.003564\n",
      "20    220829.0  768.0     0.003466\n",
      "18    219566.0  755.0     0.003427\n",
      "21    220104.0  737.0     0.003337\n",
      "\n",
      "ã€A.2 æ¸ é“æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶èˆ‡æ­£å¸¸å¸³æˆ¶çš„æ¸ é“ä½¿ç”¨æ¯”ä¾‹å·®ç•° (Prop Diff):\n",
      "              Normal_Prop  Alert_Prop  Prop_Diff\n",
      "channel_type                                    \n",
      "03               0.451734    0.789716   0.337983\n",
      "01               0.011567    0.015785   0.004218\n",
      "99               0.000289    0.000313   0.000024\n",
      "07               0.000093    0.000000  -0.000093\n",
      "05               0.000792    0.000000  -0.000792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_58584\\1926616920.py:50: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  amt_bin_pivot = amt_bin_analysis.pivot_table(index='amt_bin', columns='is_alert', values='proportion', fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ã€A.3 é‡‘é¡å€é–“æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶èˆ‡æ­£å¸¸å¸³æˆ¶çš„äº¤æ˜“é‡‘é¡åˆ†ä½ˆå·®ç•° (Prop Diff):\n",
      "          Normal_Prop  Alert_Prop  Prop_Diff\n",
      "amt_bin                                     \n",
      "<1K          0.160106    0.207158   0.047052\n",
      "1K-5K        0.344947    0.353989   0.009042\n",
      "5K-10K       0.130893    0.126358  -0.004536\n",
      ">100K        0.052783    0.045636  -0.007147\n",
      "50K-100K     0.052865    0.045323  -0.007542\n",
      "10K-50K      0.258406    0.221536  -0.036870\n",
      "\n",
      "--- B. é€²éšé—œä¿‚èˆ‡ç´°ç¯€æ¢ç´¢ (Advanced Features EDA) ---\n",
      "ã€B.1 æ”¶æ¬¾æ–¹é¡å‹æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶å°æ”¶æ¬¾æ–¹é¡å‹ (ç‰å±±/ä»–è¡Œ) çš„æ¯”ä¾‹å·®ç•°:\n",
      "              Normal_Prop  Alert_Prop  Prop_Diff\n",
      "to_acct_type                                    \n",
      "2                0.456659    0.888646   0.431986\n",
      "1                0.543341    0.111354  -0.431986\n",
      "\n",
      "ã€B.2 è‡ªæˆ‘äº¤æ˜“æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶çš„è‡ªæˆ‘äº¤æ˜“æ¯”ä¾‹ ('Y', 'N', 'UNK') å·®ç•°:\n",
      "             Normal_Prop  Alert_Prop  Prop_Diff\n",
      "is_self_txn                                    \n",
      "UNK             0.753975    0.888646   0.134671\n",
      "Y               0.032935    0.017895  -0.015041\n",
      "N               0.213090    0.093459  -0.119631\n",
      "\n",
      "ã€B.3 å¹£åˆ¥åå¥½æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶å°ä¸åŒå¹£åˆ¥çš„ä½¿ç”¨æ¯”ä¾‹å·®ç•°:\n",
      "               Normal_Prop  Alert_Prop  Prop_Diff\n",
      "currency_type                                    \n",
      "TWD               0.987962    0.994374   0.006412\n",
      "AUD               0.000169    0.000469   0.000300\n",
      "MXN               0.000002    0.000000  -0.000002\n",
      "SEK               0.000003    0.000000  -0.000003\n",
      "CHF               0.000021    0.000000  -0.000021\n",
      "\n",
      "ã€C. å¸³æˆ¶å±¤ç´šæ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶å¹³å‡æ·¨æµå‹•ï¼š\n",
      "                out_sum         in_sum     net_flow\n",
      "is_alert                                           \n",
      "0         100467.817022  100464.537911    -3.279111\n",
      "1         318696.060757  324572.011952  5875.951195\n",
      "\n",
      "(Finish) Advanced Data Exploration and Analysis.\n"
     ]
    }
   ],
   "source": [
    "# ===== 2ï¸âƒ£ Data Preparation & Exploration (Advanced EDA) - æ•¸æ“šæ¢ç´¢ =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n--- æ•¸æ“šæ¢ç´¢ (EDA) çµæœ ---\")\n",
    "\n",
    "# å‡è¨­ df_txn, df_alert å·²ç¶“è¼‰å…¥\n",
    "alert_accts = df_alert['acct'].tolist()\n",
    "df_txn['is_alert'] = df_txn['from_acct'].isin(alert_accts).astype(int)\n",
    "\n",
    "# ç¢ºä¿ txn_time æ ¼å¼æ­£ç¢ºï¼Œä¸¦æå–å°æ™‚\n",
    "try:\n",
    "    df_txn['hour'] = pd.to_datetime(df_txn['txn_time'], format='%H:%M:%S').dt.hour\n",
    "except ValueError:\n",
    "    print(\"Warning: txn_time æ ¼å¼éŒ¯èª¤ï¼Œå¯èƒ½åŒ…å«ç¼ºå¤±å€¼æˆ–éæ¨™æº–æ ¼å¼ï¼Œè¨­ç‚º 0ã€‚\")\n",
    "    df_txn['hour'] = 0\n",
    "    \n",
    "df_txn_labeled = df_txn.copy()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# A. åŸºç¤ EDA (æ™‚æ®µã€æ¸ é“ã€é‡‘é¡åˆ†ç®±)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# --- A.1 æ™‚æ®µè¦å¾‹æ¢ç´¢ ---\n",
    "hourly_analysis = df_txn_labeled.groupby(['is_alert', 'hour'])['txn_amt'].count().rename('txn_count').reset_index()\n",
    "pivot_count = hourly_analysis.pivot_table(index='hour', columns='is_alert', values='txn_count', fill_value=0)\n",
    "pivot_count.columns = ['Normal', 'Alert']\n",
    "pivot_count['Alert_Ratio'] = pivot_count['Alert'] / (pivot_count['Alert'] + pivot_count['Normal'])\n",
    "print(\"ã€A.1 æ™‚æ®µæ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶æ¯å°æ™‚äº¤æ˜“ç­†æ•¸ä½”æ¯” (Alert Ratio):\")\n",
    "print(pivot_count.sort_values(by='Alert_Ratio', ascending=False).head(8))\n",
    "\n",
    "# --- A.2 æ¸ é“åå¥½æ¢ç´¢ ---\n",
    "channel_analysis = df_txn_labeled.groupby('is_alert')['channel_type'].value_counts(normalize=True).rename('proportion').reset_index()\n",
    "channel_pivot = channel_analysis.pivot_table(index='channel_type', columns='is_alert', values='proportion', fill_value=0)\n",
    "channel_pivot.columns = ['Normal_Prop', 'Alert_Prop']\n",
    "channel_pivot['Prop_Diff'] = channel_pivot['Alert_Prop'] - channel_pivot['Normal_Prop']\n",
    "print(\"\\nã€A.2 æ¸ é“æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶èˆ‡æ­£å¸¸å¸³æˆ¶çš„æ¸ é“ä½¿ç”¨æ¯”ä¾‹å·®ç•° (Prop Diff):\")\n",
    "print(channel_pivot.sort_values(by='Prop_Diff', ascending=False).head(5))\n",
    "\n",
    "# --- A.3 é‡‘é¡å€é–“æ¢ç´¢ ---\n",
    "bins = [0, 1000, 5000, 10000, 50000, 100000, df_txn_labeled['txn_amt'].max() + 1]\n",
    "labels = ['<1K', '1K-5K', '5K-10K', '10K-50K', '50K-100K', '>100K']\n",
    "df_txn_labeled['amt_bin'] = pd.cut(df_txn_labeled['txn_amt'], bins=bins, labels=labels, right=False)\n",
    "amt_bin_analysis = (\n",
    "    df_txn_labeled.groupby('is_alert')['amt_bin']\n",
    "    .value_counts(normalize=True).rename('proportion').reset_index()\n",
    ")\n",
    "amt_bin_pivot = amt_bin_analysis.pivot_table(index='amt_bin', columns='is_alert', values='proportion', fill_value=0)\n",
    "amt_bin_pivot.columns = ['Normal_Prop', 'Alert_Prop']\n",
    "amt_bin_pivot['Prop_Diff'] = amt_bin_pivot['Alert_Prop'] - amt_bin_pivot['Normal_Prop']\n",
    "print(\"\\nã€A.3 é‡‘é¡å€é–“æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶èˆ‡æ­£å¸¸å¸³æˆ¶çš„äº¤æ˜“é‡‘é¡åˆ†ä½ˆå·®ç•° (Prop Diff):\")\n",
    "print(amt_bin_pivot.sort_values(by='Prop_Diff', ascending=False))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# B. é€²éš EDA (é—œä¿‚ã€æ™‚é–“ã€å¹£åˆ¥)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- B. é€²éšé—œä¿‚èˆ‡ç´°ç¯€æ¢ç´¢ (Advanced Features EDA) ---\")\n",
    "\n",
    "# --- B.1 äº¤æ˜“å°æ‰‹é¡å‹åå¥½ (to_acct_type) ---\n",
    "acct_type_analysis = (\n",
    "    df_txn_labeled.groupby('is_alert')['to_acct_type']\n",
    "    .value_counts(normalize=True).rename('proportion').reset_index()\n",
    ")\n",
    "acct_type_pivot = acct_type_analysis.pivot_table(\n",
    "    index='to_acct_type', columns='is_alert', values='proportion', fill_value=0\n",
    ")\n",
    "acct_type_pivot.columns = ['Normal_Prop', 'Alert_Prop']\n",
    "acct_type_pivot['Prop_Diff'] = acct_type_pivot['Alert_Prop'] - acct_type_pivot['Normal_Prop']\n",
    "\n",
    "print(\"ã€B.1 æ”¶æ¬¾æ–¹é¡å‹æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶å°æ”¶æ¬¾æ–¹é¡å‹ (ç‰å±±/ä»–è¡Œ) çš„æ¯”ä¾‹å·®ç•°:\")\n",
    "# '01': ç‰å±±ï¼› '02'ï¼šä»–è¡Œ\n",
    "print(acct_type_pivot.sort_values(by='Prop_Diff', ascending=False))\n",
    "# ğŸ’¡ æ½›åœ¨ç‰¹å¾µï¼šè­¦ç¤ºå¸³æˆ¶æ˜¯å¦æ›´å‚¾å‘æ–¼è½‰å‡ºåˆ°ä»–è¡Œ ('02')\n",
    "\n",
    "# --- B.2 è‡ªæˆ‘äº¤æ˜“ (is_self_txn) æ¯”ä¾‹ ---\n",
    "self_txn_analysis = (\n",
    "    df_txn_labeled.groupby('is_alert')['is_self_txn']\n",
    "    .value_counts(normalize=True).rename('proportion').reset_index()\n",
    ")\n",
    "self_txn_pivot = self_txn_analysis.pivot_table(\n",
    "    index='is_self_txn', columns='is_alert', values='proportion', fill_value=0\n",
    ")\n",
    "self_txn_pivot.columns = ['Normal_Prop', 'Alert_Prop']\n",
    "self_txn_pivot['Prop_Diff'] = self_txn_pivot['Alert_Prop'] - self_txn_pivot['Normal_Prop']\n",
    "\n",
    "print(\"\\nã€B.2 è‡ªæˆ‘äº¤æ˜“æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶çš„è‡ªæˆ‘äº¤æ˜“æ¯”ä¾‹ ('Y', 'N', 'UNK') å·®ç•°:\")\n",
    "print(self_txn_pivot.sort_values(by='Prop_Diff', ascending=False))\n",
    "# ğŸ’¡ æ½›åœ¨ç‰¹å¾µï¼šè­¦ç¤ºå¸³æˆ¶æ˜¯å¦æ˜é¡¯é¿é–‹è‡ªæˆ‘äº¤æ˜“æˆ–é›†ä¸­åœ¨æŸé¡\n",
    "\n",
    "# --- B.3 å¹£åˆ¥æ¢ç´¢ ---\n",
    "currency_analysis = (\n",
    "    df_txn_labeled.groupby('is_alert')['currency_type']\n",
    "    .value_counts(normalize=True).rename('proportion').reset_index()\n",
    ")\n",
    "currency_pivot = currency_analysis.pivot_table(\n",
    "    index='currency_type', columns='is_alert', values='proportion', fill_value=0\n",
    ")\n",
    "currency_pivot.columns = ['Normal_Prop', 'Alert_Prop']\n",
    "currency_pivot['Prop_Diff'] = currency_pivot['Alert_Prop'] - currency_pivot['Normal_Prop']\n",
    "\n",
    "print(\"\\nã€B.3 å¹£åˆ¥åå¥½æ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶å°ä¸åŒå¹£åˆ¥çš„ä½¿ç”¨æ¯”ä¾‹å·®ç•°:\")\n",
    "print(currency_pivot.sort_values(by='Prop_Diff', ascending=False).head(5))\n",
    "# ğŸ’¡ æ½›åœ¨ç‰¹å¾µï¼šé™¤äº† TWD å¤–ï¼Œå…¶ä»–å¹£åˆ¥ï¼ˆå¦‚ USDï¼‰çš„äº¤æ˜“æ¯”ä¾‹å·®ç•°\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# C. å¸³æˆ¶å±¤ç´šç¸½çµï¼ˆæ·¨æµå‹•ï¼‰\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹å¸³æˆ¶çš„äº¤æ˜“æ´»å‹•ï¼ˆä½œç‚º From å’Œä½œç‚º Toï¼‰\n",
    "df_from = df_txn.groupby('from_acct')['txn_amt'].agg(['count', 'sum']).rename(columns={'count': 'out_count', 'sum': 'out_sum'}).reset_index()\n",
    "df_to = df_txn.groupby('to_acct')['txn_amt'].agg(['count', 'sum']).rename(columns={'count': 'in_count', 'sum': 'in_sum'}).reset_index()\n",
    "\n",
    "# å°‡ From å’Œ To çš„æ´»å‹•åˆä½µåˆ°å–®ä¸€å¸³æˆ¶åˆ—è¡¨\n",
    "all_accts = pd.DataFrame(\n",
    "    list(set(df_txn['from_acct']) | set(df_txn['to_acct'])), \n",
    "    columns=['acct']\n",
    ")\n",
    "\n",
    "# åˆä½µè½‰å‡ºå’Œè½‰å…¥çš„çµ±è¨ˆ\n",
    "df_acct_features_eda = all_accts.merge(\n",
    "    df_from, left_on='acct', right_on='from_acct', how='left'\n",
    ").drop(columns='from_acct').merge(\n",
    "    df_to, left_on='acct', right_on='to_acct', how='left'\n",
    ").drop(columns='to_acct').fillna(0)\n",
    "\n",
    "# åŠ å…¥è­¦ç¤ºæ¨™ç±¤\n",
    "df_acct_features_eda['is_alert'] = df_acct_features_eda['acct'].isin(alert_accts).astype(int)\n",
    "\n",
    "# å»ºç«‹å¹³è¡¡æ€§ç‰¹å¾µ\n",
    "df_acct_features_eda['net_flow'] = df_acct_features_eda['in_sum'] - df_acct_features_eda['out_sum']\n",
    "\n",
    "print(\"\\nã€C. å¸³æˆ¶å±¤ç´šæ¢ç´¢ã€‘è­¦ç¤ºå¸³æˆ¶å¹³å‡æ·¨æµå‹•ï¼š\")\n",
    "print(df_acct_features_eda.groupby('is_alert')[['out_sum', 'in_sum', 'net_flow']].mean())\n",
    "\n",
    "print(\"\\n(Finish) Advanced Data Exploration and Analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93470e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Caching) å¿«å–æª”æ¡ˆæœªæ‰¾åˆ°ï¼Œæ­£åœ¨åŸ·è¡Œå®Œæ•´çš„ç‰¹å¾µå·¥ç¨‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_58584\\3136033161.py:191: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: roll_window_agg(x, window))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_58584\\3136033161.py:191: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: roll_window_agg(x, window))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Finish) Advanced Feature Engineering.\n",
      "(Caching) å·²å°‡ df_X å„²å­˜è‡³å¿«å–æª”æ¡ˆï¼šdf_X_features_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# å‡è¨­ df_txn, df_alert, df_test (df_submit), calculate_entropy å·²ç¶“åœ¨ç’°å¢ƒä¸­å®šç¾©\n",
    "# æ³¨æ„ï¼šåœ¨ acct.ipynb ä¸­ï¼Œæ‚¨ä½¿ç”¨çš„æ¸¬è©¦é›†æª”æ¡ˆåç‚º df_testã€‚\n",
    "df_submit = df_test # çµ±ä¸€è®Šæ•¸åç¨±ä»¥ä¾¿æ–¼å¾ŒçºŒä»£ç¢¼é‚è¼¯\n",
    "\n",
    "# ===== 3ï¸âƒ£ ADVANCED Feature Engineering (åŸºæ–¼æ¢ç´¢çµæœ) - é‡æ–°æ•´ç†ç‰ˆ =====\n",
    "\n",
    "CACHE_FILE_PATH = 'df_X_features_cache.pkl' # å®šç¾©å¿«å–æª”æ¡ˆè·¯å¾‘\n",
    "\n",
    "# --- å¿«å–æª¢æŸ¥é‚è¼¯ ---\n",
    "if os.path.exists(CACHE_FILE_PATH):\n",
    "    print(f\"(Caching) åµæ¸¬åˆ°å¿«å–æª”æ¡ˆï¼š{CACHE_FILE_PATH}ã€‚æ­£åœ¨è¼‰å…¥...\")\n",
    "    df_X = pd.read_pickle(CACHE_FILE_PATH)\n",
    "    print(\"(Finish) å¿«å–è¼‰å…¥å®Œæˆã€‚\")\n",
    "else:\n",
    "    print(\"(Caching) å¿«å–æª”æ¡ˆæœªæ‰¾åˆ°ï¼Œæ­£åœ¨åŸ·è¡Œå®Œæ•´çš„ç‰¹å¾µå·¥ç¨‹...\")\n",
    "    \n",
    "    # --- é å…ˆè™•ç†èˆ‡æº–å‚™ ---\n",
    "    try:\n",
    "        # ç¢ºä¿ txn_time è™•ç†å®Œæˆï¼Œæå– hour ç‰¹å¾µ\n",
    "        if 'hour' not in df_txn.columns:\n",
    "             # å‡è¨­ txn_time æ˜¯ 'HH:MM:SS' æ ¼å¼çš„å­—ä¸²ï¼Œéœ€èª¿æ•´ç¨‹å¼ç¢¼ä»¥åŒ¹é…æ‚¨çš„å¯¦éš›æ•¸æ“šé¡å‹\n",
    "             # ç”±æ–¼åŸå§‹ç­†è¨˜æœ¬ä¸­æ²’æœ‰çœ‹åˆ° txn_time è½‰æ›çš„ä»£ç¢¼ï¼Œé€™è£¡å…ˆå‡è¨­å®ƒéœ€è¦è¢«è™•ç†\n",
    "             # å¦‚æœ txn_time æ˜¯æ•¸å€¼å‹ï¼Œè«‹èª¿æ•´ç‚ºæå–å…¶å°æ™‚éƒ¨åˆ†\n",
    "             df_txn['hour'] = pd.to_datetime(df_txn['txn_time'], format='%H:%M:%S').dt.hour\n",
    "    except Exception as e:\n",
    "        # å¦‚æœ txn_time è™•ç†å¤±æ•—ï¼Œå‰‡éœ€è¦ä½¿ç”¨å…¶ä»–æ–¹å¼è™•ç†æ™‚é–“\n",
    "        print(f\"Warning: txn_time è½‰ hour ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ txn_time æ ¼å¼ã€‚\")\n",
    "        \n",
    "    # --- 3.1 å¯¦æ–½åš´æ ¼çš„æ™‚é–“æˆªæ­¢æ§åˆ¶ (Temporal Cutoff) ---\n",
    "    \n",
    "    # 1. æ‰¾å‡ºæ‰€æœ‰è­¦ç¤ºå¸³æˆ¶çš„ã€Œæœ€æ—©ã€è­¦ç¤ºæ—¥æœŸï¼Œä½œç‚ºæˆªæ­¢æ—¥æœŸ\n",
    "    acct_cutoff_map = (\n",
    "        df_alert.groupby('acct')['event_date'].min()\n",
    "        .reset_index().rename(columns={'event_date': 'cutoff_date'})\n",
    "    )\n",
    "\n",
    "    # å°‡æˆªæ­¢æ—¥æœŸåˆä½µåˆ°äº¤æ˜“æ•¸æ“š (ä½œç‚º from_acct å’Œ to_acct)\n",
    "    df_txn_merged = df_txn.merge(\n",
    "        acct_cutoff_map, left_on='from_acct', right_on='acct', how='left', suffixes=('', '_from')\n",
    "    ).drop(columns=['acct_from'], errors='ignore').rename(columns={'cutoff_date': 'from_cutoff_date'})\n",
    "\n",
    "    df_txn_merged = df_txn_merged.merge(\n",
    "        acct_cutoff_map, left_on='to_acct', right_on='acct', how='left', suffixes=('', '_to')\n",
    "    ).drop(columns=['acct_to'], errors='ignore').rename(columns={'cutoff_date': 'to_cutoff_date'})\n",
    "\n",
    "    # ğŸŒŸ å»ºè­°å„ªåŒ–é» 1ï¼šå‘é‡åŒ–æ™‚é–“æˆªæ­¢éæ¿¾ ğŸŒŸ\n",
    "    # éæ¿¾äº¤æ˜“ï¼šäº¤æ˜“æ—¥æœŸå¿…é ˆæ—©æ–¼ from_acct **å’Œ** to_acct çš„è­¦ç¤ºæ—¥æœŸ\n",
    "    \n",
    "    # è™•ç†æœªè­¦ç¤ºå¸³æˆ¶ï¼šè‹¥ç„¡è­¦ç¤ºæ—¥æœŸ (NaN)ï¼Œè¦–ç‚ºæˆªæ­¢æ—¥æœŸç„¡é™å¤§ (ä¾‹å¦‚æ•¸æ“šé›†æœ€å¤§æ—¥æœŸ+1)\n",
    "    MAX_DATE = df_txn['txn_date'].max() + 1 \n",
    "    df_txn_merged['from_cutoff_date'] = df_txn_merged['from_cutoff_date'].fillna(MAX_DATE)\n",
    "    df_txn_merged['to_cutoff_date'] = df_txn_merged['to_cutoff_date'].fillna(MAX_DATE)\n",
    "    \n",
    "    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œéæ¿¾ï¼Œé€Ÿåº¦æ›´å¿«\n",
    "    df_txn_cutoff = df_txn_merged[\n",
    "        (df_txn_merged['txn_date'] < df_txn_merged['from_cutoff_date']) & \n",
    "        (df_txn_merged['txn_date'] < df_txn_merged['to_cutoff_date'])\n",
    "    ].copy()\n",
    "    \n",
    "    # æ¸…ç†æˆªæ­¢æ—¥æœŸæ¬„ä½\n",
    "    df_txn_cutoff.drop(columns=['from_cutoff_date', 'to_cutoff_date'], inplace=True)\n",
    "    \n",
    "    # ç²å–æ‰€æœ‰ç›¸é—œå¸³æˆ¶ (åŒ…å«ç™¼é€æ–¹ã€æ¥æ”¶æ–¹ã€æäº¤æ¸…å–®ä¸­çš„å¸³æˆ¶)\n",
    "    all_accts = pd.concat([\n",
    "        df_txn['from_acct'], df_txn['to_acct'], df_submit['acct']\n",
    "    ]).drop_duplicates().reset_index(drop=True).to_frame(name='acct')\n",
    "\n",
    "    \n",
    "    # --- 3.2 åŸºç¤äº¤æ˜“çµ±è¨ˆ (Base Aggregation) ---\n",
    "    \n",
    "    # ç™¼é€çµ±è¨ˆ (Send Features)\n",
    "    df_send_features = (\n",
    "        df_txn_cutoff.groupby('from_acct')['txn_amt']\n",
    "        .agg(\n",
    "            max_send_amt='max', min_send_amt='min', avg_send_amt='mean',\n",
    "            total_send_amt='sum', send_count='count'\n",
    "        )\n",
    "    ).reset_index().rename(columns={'from_acct': 'acct'})\n",
    "\n",
    "    # æ¥æ”¶çµ±è¨ˆ (Receive Features) - **åŸºæ–¼å·²æˆªæ­¢çš„æ•¸æ“š**\n",
    "    df_recv_features = (\n",
    "        df_txn_cutoff.groupby('to_acct')['txn_amt']\n",
    "        .agg(\n",
    "            max_recv_amt='max', min_recv_amt='min', avg_recv_amt='mean',\n",
    "            total_recv_amt='sum', recv_count='count'\n",
    "        )\n",
    "    ).reset_index().rename(columns={'to_acct': 'acct'})\n",
    "\n",
    "\n",
    "    # --- 3.3 ç†µç‰¹å¾µ (Entropy Features) ---\n",
    "    \n",
    "    # ç™¼é€æ–¹å°æ¥æ”¶æ–¹çš„ç†µ (è¡¡é‡äº¤æ˜“å°æ‰‹å¤šæ¨£æ€§)\n",
    "    df_receiver_entropy = (\n",
    "        df_txn_cutoff.groupby('from_acct')['to_acct']\n",
    "        .apply(calculate_entropy)\n",
    "        .rename('sender_receiver_entropy')\n",
    "        .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    )\n",
    "\n",
    "    # ç™¼é€æ–¹å°æ¸ é“çš„ç†µ (è¡¡é‡æ¸ é“ä½¿ç”¨å¤šæ¨£æ€§)\n",
    "    df_channel_entropy = (\n",
    "        df_txn_cutoff.groupby('from_acct')['channel_type']\n",
    "        .apply(calculate_entropy)\n",
    "        .rename('sender_channel_entropy')\n",
    "        .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # --- 3.4 æ™‚æ®µèˆ‡æ¸ é“æ¯”ä¾‹ç‰¹å¾µ (Hourly & Channel Ratios) ---\n",
    "    \n",
    "    # ğŸŒŸ å»ºè­°å„ªåŒ–é» 2ï¼šæ™‚æ®µç‰¹å¾µèª¿æ•´ ğŸŒŸ\n",
    "    # æ ¹æ“š EDA çµæœï¼Œè­¦ç¤ºæ¯”æœ€é«˜çš„æ™‚æ®µæ˜¯ 4, 1, 23, 3 \n",
    "    # ç‚ºäº†æ•æ‰é€™äº›ç•°å¸¸æ™‚æ®µçš„äº¤æ˜“ï¼Œæˆ‘å€‘ä½¿ç”¨é€™äº›å°æ™‚ä½œç‚ºç‰¹å¾µ\n",
    "    EARLY_MORNING_HOUR = [1, 3, 4, 23] # èª¿æ•´ç‚º EDA ç™¼ç¾çš„é«˜é¢¨éšªæ™‚æ®µ\n",
    "    df_hourly = (\n",
    "        df_txn_cutoff.groupby('from_acct')\n",
    "        .agg(\n",
    "            high_risk_hour_count=('hour', lambda x: (x.isin(EARLY_MORNING_HOUR)).sum()),\n",
    "        )\n",
    "    ).reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    \n",
    "    # åˆä½µç­†æ•¸ï¼Œè¨ˆç®—æ¯”ä¾‹\n",
    "    df_hourly = df_hourly.merge(df_send_features[['acct', 'send_count']], on='acct', how='left')\n",
    "    df_hourly['high_risk_hour_ratio'] = df_hourly['high_risk_hour_count'] / df_hourly['send_count'].replace(0, np.nan)\n",
    "    df_hourly.drop(columns=['high_risk_hour_count', 'send_count'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # æ¸ é“æ¯”ä¾‹ One-Hot Encoding (é‚è¼¯ä¿ç•™ï¼Œå‘½åæ›´æ¸…æ™°)\n",
    "    df_channel_ratio_raw = (\n",
    "        df_txn_cutoff.groupby('from_acct')['channel_type']\n",
    "        .value_counts(normalize=True).rename('channel_proportion')\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_channel_ratio = df_channel_ratio_raw.pivot_table(\n",
    "        index='from_acct', columns='channel_type', values='channel_proportion'\n",
    "    ).fillna(0).reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    \n",
    "    # ç¢ºä¿æ¬„ä½åç¨±ç¬¦åˆæ¨¡å‹é æœŸ (ä¾‹å¦‚ channel_1_ratio, channel_2_ratio ç­‰)\n",
    "    df_channel_ratio.columns = [\n",
    "        f'channel_{int(c)}_ratio' if isinstance(c, (int, float)) and not np.isnan(c) and c != 'acct' else c \n",
    "        for c in df_channel_ratio.columns\n",
    "    ]\n",
    "    \n",
    "    # ä½åƒ¹å€¼äº¤æ˜“æ¯”ä¾‹ (<1K Smurfing Ratio)\n",
    "    LOW_VALUE_THRESHOLD = 1000\n",
    "    df_low_value_count = (\n",
    "        df_txn_cutoff[df_txn_cutoff['txn_amt'] < LOW_VALUE_THRESHOLD]\n",
    "        .groupby('from_acct')['txn_amt'].count().rename('low_value_count')\n",
    "        .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "    )\n",
    "    df_low_value_ratio = df_low_value_count.merge(\n",
    "        df_send_features[['acct', 'send_count']], on='acct', how='left'\n",
    "    )\n",
    "    df_low_value_ratio['low_value_txn_ratio'] = (\n",
    "        df_low_value_ratio['low_value_count'] / df_low_value_ratio['send_count'].replace(0, np.nan)\n",
    "    )\n",
    "    df_low_value_ratio.drop(columns=['low_value_count', 'send_count'], inplace=True)\n",
    "\n",
    "\n",
    "    # --- 3.5 æ»¾å‹•çª—å£ç‰¹å¾µ (7å¤© / 30å¤©å¿«ç…§) ---\n",
    "    \n",
    "    WINDOWS = [7, 30]\n",
    "    df_rolling_features = all_accts[['acct']].copy()\n",
    "\n",
    "    for window in WINDOWS:\n",
    "        # ğŸŒŸ å»ºè­°å„ªåŒ–é» 3ï¼šæ»¾å‹•çª—å£çš„å¯¦éš›æ‡‰ç”¨ ğŸŒŸ\n",
    "        # ç”±æ–¼æˆ‘å€‘å·²ç¶“å¯¦æ–½äº† Temporal Cutoffï¼Œé€™è£¡è¨ˆç®—çš„æ»¾å‹•çª—å£\n",
    "        # å…¶å¯¦æ˜¯ç›¸å°æ–¼ã€Œè­¦ç¤ºæ—¥æœŸå‰çš„æœ€å¾Œä¸€ç­†äº¤æ˜“ã€çš„çª—å£ã€‚\n",
    "        # å¦‚æœè¦è¨ˆç®—ç›¸å°äºæ¯å€‹å¸³æˆ¶çš„ã€Œæˆªæ­¢æ—¥ã€çš„çª—å£ï¼Œéœ€è¦æ›´è¤‡é›œçš„ Grouped Time Series Aggregation\n",
    "        # ä½†åœ¨ç¢ºä¿æˆªæ­¢çš„ df_txn_cutoff ä¸Šï¼Œä»¥ä¸‹ç°¡å–®èšåˆä»æ˜¯æœ‰æ•ˆçš„ç‰¹å¾µã€‚\n",
    "        \n",
    "        def roll_window_agg(group, window):\n",
    "            # åŸºæ–¼æœ€å¾Œä¸€ç­†äº¤æ˜“æ—¥æœŸçš„ç°¡å–®çª—å£\n",
    "            latest_date = group['txn_date'].max() \n",
    "            start_date = latest_date - window # window å·²ç¶“æ˜¯å¤©æ•¸ (7, 30)\n",
    "            window_data = group[group['txn_date'] > start_date] \n",
    "            \n",
    "            return pd.Series({\n",
    "                f'txn_count_{window}d': window_data.shape[0],\n",
    "                f'total_amt_{window}d': window_data['txn_amt'].sum(),\n",
    "                f'avg_amt_{window}d': window_data['txn_amt'].mean()\n",
    "            })\n",
    "\n",
    "        # é€™è£¡éœ€è¦å‚³é window åƒæ•¸ï¼Œå°‡ apply å…§çš„ lambda ä¿®æ­£\n",
    "        df_win = (\n",
    "            df_txn_cutoff.groupby('from_acct')\n",
    "            .apply(lambda x: roll_window_agg(x, window))\n",
    "            .reset_index().rename(columns={'from_acct': 'acct'})\n",
    "        )\n",
    "        \n",
    "        df_rolling_features = df_rolling_features.merge(df_win, on='acct', how='left')\n",
    "\n",
    "\n",
    "    # --- 3.6 æ·¨æµå‹•èˆ‡æ´»å‹•åŠ é€Ÿåº¦ç‰¹å¾µ (EDA Insights) ---\n",
    "\n",
    "    df_eda_ratio = all_accts.copy()\n",
    "\n",
    "    # åˆä½µ total_send_amt å’Œ total_recv_amt\n",
    "    df_eda_ratio = df_eda_ratio.merge(df_send_features[['acct', 'total_send_amt', 'send_count']], on='acct', how='left').fillna(0)\n",
    "    df_eda_ratio = df_eda_ratio.merge(df_recv_features[['acct', 'total_recv_amt', 'recv_count']], on='acct', how='left').fillna(0)\n",
    "\n",
    "    # ğŸŒŸ æ ¸å¿ƒç‰¹å¾µï¼šæ·¨æµå‹•é‡‘é¡ (Net Flow Amount) ğŸŒŸ\n",
    "    df_eda_ratio['net_flow_amt'] = df_eda_ratio['total_recv_amt'] - df_eda_ratio['total_send_amt'] \n",
    "\n",
    "    # ğŸŒŸ æ ¸å¿ƒç‰¹å¾µï¼šæ·¨æµå‹•æ¯” (Net Flow Ratio) ğŸŒŸ\n",
    "    df_eda_ratio['net_flow_ratio'] = (\n",
    "        df_eda_ratio['total_recv_amt'] - df_eda_ratio['total_send_amt'] \n",
    "    ) / (\n",
    "        df_eda_ratio['total_send_amt'] + df_eda_ratio['total_recv_amt'] + 1\n",
    "    ) # +1 é˜²æ­¢é™¤ä»¥é›¶\n",
    "\n",
    "    # äº¤æ˜“æ´»å‹•åŠ é€Ÿåº¦ (Velocity)\n",
    "    df_velocity = df_rolling_features[['acct', 'txn_count_7d', 'txn_count_30d']].copy()\n",
    "    df_velocity['txn_velocity'] = (\n",
    "        df_velocity['txn_count_7d'] / (df_velocity['txn_count_30d'] + 1) # +1 é˜²æ­¢é™¤ä»¥é›¶\n",
    "    )\n",
    "    df_velocity.drop(columns=['txn_count_7d', 'txn_count_30d'], inplace=True)\n",
    "    \n",
    "    # åˆä½µæ‰€æœ‰ EDA ç›¸é—œç‰¹å¾µ\n",
    "    df_eda_ratio = df_eda_ratio.merge(df_velocity, on='acct', how='left')\n",
    "    df_eda_ratio.drop(columns=['total_send_amt', 'total_recv_amt', 'send_count', 'recv_count'], inplace=True)\n",
    "\n",
    "\n",
    "    # --- 3.7 æœ€çµ‚ç‰¹å¾µåˆä½µèˆ‡æ¸…ç† ---\n",
    "    \n",
    "    # ä»¥ all_accts ç‚ºåŸºç¤åˆä½µæ‰€æœ‰ç‰¹å¾µ\n",
    "    df_X = all_accts.copy()\n",
    "    \n",
    "    df_X = df_X.merge(df_send_features, on='acct', how='left').fillna(0)\n",
    "    df_X = df_X.merge(df_recv_features, on='acct', how='left').fillna(0)\n",
    "    df_X = df_X.merge(df_receiver_entropy, on='acct', how='left').fillna(0)\n",
    "    df_X = df_X.merge(df_channel_entropy, on='acct', how='left').fillna(0)\n",
    "    df_X = df_X.merge(df_hourly, on='acct', how='left').fillna(0)\n",
    "    df_X = df_X.merge(df_channel_ratio, on='acct', how='left').fillna(0)\n",
    "    df_X = df_X.merge(df_low_value_ratio, on='acct', how='left').fillna(0) \n",
    "    df_X = df_X.merge(df_rolling_features.drop(columns=['txn_count_30d', 'txn_count_7d']), on='acct', how='left').fillna(0) \n",
    "    df_X = df_X.merge(df_eda_ratio, on='acct', how='left').fillna(0)\n",
    "    \n",
    "    # è£œä¸Šå¸³æˆ¶é¡å‹ (is_esun) - é—œéµçš„éœæ…‹ç‰¹å¾µ\n",
    "    df_from = df_txn[['from_acct', 'from_acct_type']].rename(columns={'from_acct':'acct','from_acct_type':'is_esun'})\n",
    "    df_to = df_txn[['to_acct', 'to_acct_type']].rename(columns={'to_acct':'acct','to_acct_type':'is_esun'})\n",
    "    df_acc = pd.concat([df_from, df_to], ignore_index=True).drop_duplicates(subset=['acct']).reset_index(drop=True)\n",
    "    df_X = pd.merge(df_X, df_acc, on='acct', how='left').fillna(-1) \n",
    "\n",
    "    print(\"(Finish) Advanced Feature Engineering.\")\n",
    "\n",
    "    # --- å„²å­˜å¿«å– ---\n",
    "    df_X.to_pickle(CACHE_FILE_PATH)\n",
    "    print(f\"(Caching) å·²å°‡ df_X å„²å­˜è‡³å¿«å–æª”æ¡ˆï¼š{CACHE_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cf2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é›†æ¨£æœ¬æ•¸ï¼šç¸½æ•¸=328988, æ­£å¸¸(0)=327984, è­¦ç¤º(1)=1004\n",
      "LGBM æ¬Šé‡ (scale_pos_weight)ï¼š326.68\n",
      "\n",
      "--- 3.1 å•Ÿå‹• 5 æŠ˜ Stratified K-Fold è¨“ç·´ (ç¢ºä¿è­¦ç¤ºæ¯”ä¾‹ä¸€è‡´) ---\n",
      "Fold 1/5...\n",
      "[LightGBM] [Info] Number of positive: 803, number of negative: 262387\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6636\n",
      "[LightGBM] [Info] Number of data points in the train set: 263190, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003051 -> initscore=-5.789221\n",
      "[LightGBM] [Info] Start training from score -5.789221\n",
      "Fold 2/5...\n",
      "[LightGBM] [Info] Number of positive: 803, number of negative: 262387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6633\n",
      "[LightGBM] [Info] Number of data points in the train set: 263190, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003051 -> initscore=-5.789221\n",
      "[LightGBM] [Info] Start training from score -5.789221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 3/5...\n",
      "[LightGBM] [Info] Number of positive: 803, number of negative: 262387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6626\n",
      "[LightGBM] [Info] Number of data points in the train set: 263190, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003051 -> initscore=-5.789221\n",
      "[LightGBM] [Info] Start training from score -5.789221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 4/5...\n",
      "[LightGBM] [Info] Number of positive: 804, number of negative: 262387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6634\n",
      "[LightGBM] [Info] Number of data points in the train set: 263191, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003055 -> initscore=-5.787977\n",
      "[LightGBM] [Info] Start training from score -5.787977\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Fold 5/5...\n",
      "[LightGBM] [Info] Number of positive: 803, number of negative: 262388\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6635\n",
      "[LightGBM] [Info] Number of data points in the train set: 263191, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003051 -> initscore=-5.789225\n",
      "[LightGBM] [Info] Start training from score -5.789225\n",
      "--- 3.2 K-Fold è¨“ç·´å®Œæˆ ---\n",
      "\n",
      "--- 4.1 å°‹æ‰¾æœ€å¤§åŒ– F1-score çš„æœ€ä½³é–¾å€¼ (ä½¿ç”¨ OOF é æ¸¬) ---\n",
      "\n",
      "[F1-score å„ªåŒ–] æˆåŠŸåœ¨ OOF æ•¸æ“šæ‰¾åˆ°æœ€ä½³é–¾å€¼:\n",
      " Â Max F1-score on OOF Data: 0.2681\n",
      " Â Optimal Probability Threshold: 0.9133\n",
      "-----------------------------------\n",
      "ã€OOF è©•ä¼°å ±å‘Šã€‘ä½¿ç”¨æœ€ä½³é–¾å€¼\n",
      "\n",
      "--- OOF æ··æ·†çŸ©é™£ (Confusion Matrix) ---\n",
      "[[327695    289]\n",
      " [   804    200]]\n",
      "\n",
      "--- OOF åˆ†é¡å ±å‘Š (Classification Report) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Normal (0)       1.00      1.00      1.00    327984\n",
      "   Alert (1)       0.41      0.20      0.27      1004\n",
      "\n",
      "    accuracy                           1.00    328988\n",
      "   macro avg       0.70      0.60      0.63    328988\n",
      "weighted avg       1.00      1.00      1.00    328988\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "--- 5.1 æ‡‰ç”¨æœ€ä½³é–¾å€¼é€²è¡Œæœ€çµ‚é æ¸¬ ---\n",
      "(Finish) Robust Modeling and F1-score Optimization.\n"
     ]
    }
   ],
   "source": [
    "# ===== 4ï¸âƒ£ Train/Test Split & Ensemble Modeling (5 Models) =====\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. æº–å‚™æ•¸æ“šèˆ‡åˆ†å±¤åŠƒåˆ†\n",
    "# ----------------------------------------------------\n",
    "df_X['label'] = df_X['acct'].isin(df_alert['acct']).astype(int)\n",
    "\n",
    "X_train = df_X[(~df_X['acct'].isin(df_test['acct'])) & (df_X['is_esun']==1)].copy()\n",
    "y_train = X_train['label']\n",
    "X_test = df_X[df_X['acct'].isin(df_test['acct'])].copy()\n",
    "\n",
    "feature_cols = [\n",
    "    col for col in X_train.columns \n",
    "    if col not in ['acct', 'is_esun', 'label', 'anomaly_score']\n",
    "]\n",
    "X_train_features = X_train[feature_cols].fillna(0)\n",
    "X_test_features = X_test[feature_cols].fillna(0)\n",
    "\n",
    "# æ¨™æº–åŒ–ï¼ˆå° Logistic Regression / SVM æ•ˆæœæ›´å¥½ï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. è™•ç†é¡åˆ¥ä¸å¹³è¡¡\n",
    "# ----------------------------------------------------\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight_value = neg_count / pos_count \n",
    "print(f\"è¨“ç·´é›†æ¨£æœ¬æ•¸ï¼šç¸½æ•¸={len(y_train)}, æ­£å¸¸(0)={neg_count}, è­¦ç¤º(1)={pos_count}\")\n",
    "print(f\"å»ºè­°æ¬Šé‡ (scale_pos_weight)ï¼š{scale_pos_weight_value:.2f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. K-Fold Ensemble è¨“ç·´\n",
    "# ----------------------------------------------------\n",
    "N_SPLITS = 5\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros(X_train_features.shape[0])\n",
    "test_preds = np.zeros(X_test_features.shape[0])\n",
    "\n",
    "print(f\"\\n--- å•Ÿå‹• {N_SPLITS}-Fold Ensemble æ¨¡å‹è¨“ç·´ ---\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_features, y_train)):\n",
    "    print(f\"Fold {fold+1}/{N_SPLITS} ...\")\n",
    "\n",
    "    X_tr, X_val = X_train_features.iloc[train_idx], X_train_features.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    X_tr_s, X_val_s = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "\n",
    "    # ----- å„æ¨¡å‹åˆå§‹åŒ– -----\n",
    "    model_lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "    model_svm = LinearSVC(class_weight='balanced', random_state=42)\n",
    "    model_dt = DecisionTreeClassifier(max_depth=8, class_weight='balanced', random_state=42)\n",
    "    model_xgb = xgb.XGBClassifier(\n",
    "        objective='binary:logistic', eval_metric='auc', \n",
    "        scale_pos_weight=scale_pos_weight_value,\n",
    "        random_state=42, n_estimators=1000, learning_rate=0.05,\n",
    "        max_depth=6, subsample=0.8, colsample_bytree=0.8\n",
    "    )\n",
    "    model_lgb = lgb.LGBMClassifier(\n",
    "        objective='binary', metric='auc',\n",
    "        scale_pos_weight=scale_pos_weight_value,\n",
    "        random_state=42, n_estimators=1000, learning_rate=0.05,\n",
    "        num_leaves=32, max_depth=8, subsample=0.8, colsample_bytree=0.8\n",
    "    )\n",
    "\n",
    "    # ----- æ¨¡å‹è¨“ç·´ -----\n",
    "    model_lr.fit(X_tr_s, y_tr)\n",
    "    model_svm.fit(X_tr_s, y_tr)\n",
    "    model_dt.fit(X_tr, y_tr)\n",
    "    model_xgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    model_lgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "    # ----- é©—è­‰é›†é æ¸¬ -----\n",
    "    pred_lr = model_lr.predict_proba(X_val_s)[:,1]\n",
    "    pred_svm = model_svm.decision_function(X_val_s)\n",
    "    pred_svm = (pred_svm - pred_svm.min()) / (pred_svm.max() - pred_svm.min() + 1e-6)  # æ­£è¦åŒ–ç‚ºæ©Ÿç‡\n",
    "    pred_dt = model_dt.predict_proba(X_val)[:,1]\n",
    "    pred_xgb = model_xgb.predict_proba(X_val)[:,1]\n",
    "    pred_lgb = model_lgb.predict_proba(X_val)[:,1]\n",
    "\n",
    "    # ----- å–äº”æ¨¡å‹å¹³å‡æ©Ÿç‡ -----\n",
    "    pred_avg = (pred_lr + pred_svm + pred_dt + pred_xgb + pred_lgb) / 5\n",
    "    oof_preds[val_idx] = pred_avg\n",
    "\n",
    "    # ----- æ¸¬è©¦é›†é æ¸¬å¹³å‡ -----\n",
    "    test_lr = model_lr.predict_proba(X_test_scaled)[:,1]\n",
    "    test_svm = model_svm.decision_function(X_test_scaled)\n",
    "    test_svm = (test_svm - test_svm.min()) / (test_svm.max() - test_svm.min() + 1e-6)\n",
    "    test_dt = model_dt.predict_proba(X_test_features)[:,1]\n",
    "    test_xgb = model_xgb.predict_proba(X_test_features)[:,1]\n",
    "    test_lgb = model_lgb.predict_proba(X_test_features)[:,1]\n",
    "\n",
    "    test_preds += (test_lr + test_svm + test_dt + test_xgb + test_lgb) / (5 * N_SPLITS)\n",
    "\n",
    "print(\"\\n--- Ensemble è¨“ç·´å®Œæˆ ---\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. å°‹æ‰¾æœ€å¤§åŒ– F1-score çš„æœ€ä½³é–¾å€¼\n",
    "# ----------------------------------------------------\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, oof_preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "fscore[np.isnan(fscore)] = 0\n",
    "ix = np.argmax(fscore)\n",
    "best_f1 = fscore[ix]\n",
    "best_threshold = thresholds[ix - 1] if ix > 0 else thresholds[ix]\n",
    "print(f\"\\næœ€ä½³ F1-score: {best_f1:.4f}, æœ€ä½³é–¾å€¼: {best_threshold:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. OOF è©•ä¼°èˆ‡æœ€çµ‚é æ¸¬\n",
    "# ----------------------------------------------------\n",
    "y_oof_pred = (oof_preds >= best_threshold).astype(int)\n",
    "print(\"\\n--- OOF æ··æ·†çŸ©é™£ ---\")\n",
    "print(confusion_matrix(y_train, y_oof_pred))\n",
    "print(\"\\n--- OOF åˆ†é¡å ±å‘Š ---\")\n",
    "print(classification_report(y_train, y_oof_pred, target_names=['Normal (0)', 'Alert (1)'], zero_division=0))\n",
    "\n",
    "# æœ€çµ‚æ¸¬è©¦é›†é æ¸¬\n",
    "y_pred = (test_preds >= best_threshold).astype(int)\n",
    "print(\"\\n(Finish) Ensemble Modeling with F1 Optimization å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Finish) Output saved to result_advanced_eda.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== 5ï¸âƒ£ Output CSV =====\n",
    "df_pred = pd.DataFrame({\n",
    "    'acct': X_test['acct'].values,\n",
    "    'label': y_pred \n",
    "})\n",
    "\n",
    "df_out = df_test[['acct']].merge(df_pred, on='acct', how='left')\n",
    "out_path = \"result_advanced_eda.csv\"\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"(Finish) Output saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237607e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d597a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
